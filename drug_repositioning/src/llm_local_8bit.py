import os
import json
import torch
import math
import argparse
import pandas as pd
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# === 1. é…ç½® ===
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# === å¢å¼ºç‰ˆ System Prompt (é˜²æ­¢æ€è€ƒæ¨¡å¼) ===
SYSTEM_PROMPT = """You are an expert in drug repurposing. Your task is to predict whether a specific drug has any potential therapeutic effect (including off-label, investigational, or mechanistic plausibility) for a specific disease.
Respond with exactly ONE word: "Yes" or "No".
- Answer "Yes" if there is any biological rationale, clinical evidence, or shared pathway suggesting potential efficacy.
- Answer "No" only if they are completely unrelated or contraindicated.
"""

USER_PROMPT_TEMPLATE = """Target Pair:
Drug: "{drug_name}"
Disease: "{disease_name}"
"""


def safe_token_match(token_str):
    return ''.join([c for c in token_str if c.isalpha()]).lower()


def get_local_logprobs(model, tokenizer, drug, disease, top_k=20):
    """
    æœ¬åœ°æ¨¡å‹æ ¸å¿ƒæ¨ç†å‡½æ•°
    """
    user_content = USER_PROMPT_TEMPLATE.format(drug_name=drug, disease_name=disease)

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_content}
    ]

    # æ„é€  Prompt
    try:
        text_input = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False
        )
    except TypeError:
        text_input = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

    inputs = tokenizer(text_input, return_tensors="pt").to(DEVICE)

    # æ¨ç†ç”Ÿæˆï¼ˆåªç”Ÿæˆ 1 ä¸ª tokenï¼‰
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=1,
            return_dict_in_generate=True,
            output_scores=True,  # å¿…é¡»å¼€å¯
            temperature=0.001,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )

    # è·å– Logits
    first_token_logits = outputs.scores[0][0]
    log_probs = torch.nn.functional.log_softmax(first_token_logits, dim=-1)

    # å– Top K
    top_values, top_indices = torch.topk(log_probs, top_k)

    # è§£ç 
    sum_prob_yes = 0.0
    sum_prob_no = 0.0

    raw_token_id = outputs.sequences[0][-1].item()
    raw_response = tokenizer.decode(raw_token_id).strip()

    for score, token_id in zip(top_values, top_indices):
        score = score.item()
        token_str = tokenizer.decode(token_id)

        clean_token = safe_token_match(token_str)
        prob_linear = math.exp(score)

        if clean_token == "yes":
            sum_prob_yes += prob_linear
        elif clean_token == "no":
            sum_prob_no += prob_linear

    sum_prob_yes = max(sum_prob_yes, 1e-10)
    sum_prob_no = max(sum_prob_no, 1e-10)

    pred_prob = sum_prob_yes / (sum_prob_yes + sum_prob_no)

    return pred_prob, raw_response


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_path", required=True, help="Local path to model directory")
    parser.add_argument("--input_file", required=True)
    parser.add_argument("--output_jsonl", required=True)
    parser.add_argument("--limit", type=int, default=None)
    args = parser.parse_args()

    print(f"ğŸš€ Loading model from: {args.model_path} ...")
    print("âš™ï¸  Configuring for 8-bit quantization...")

    # === ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼š8-bit é‡åŒ–é…ç½® ===
    quantization_config = BitsAndBytesConfig(
        load_in_8bit=True,  # å¼€å¯ 8-bit
        # llm_int8_threshold=6.0 # (å¯é€‰) ç¦»ç¾¤å€¼é˜ˆå€¼ï¼Œé»˜è®¤ä¸º6.0ï¼Œé€šå¸¸ä¸éœ€è¦æ”¹
    )

    # åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)

    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        args.model_path,
        device_map="auto",
        quantization_config=quantization_config,  # æ³¨å…¥ 8-bit é…ç½®
        trust_remote_code=True
    )

    print(f"âœ… Model loaded successfully (8-bit)!")
    print(f"   Memory footprint should be around ~10GB.")

    # è¯»å–æ•°æ®
    data = []
    if args.input_file.endswith('.csv'):
        try:
            df = pd.read_csv(args.input_file)
            data = df.to_dict('records')
        except pd.errors.EmptyDataError:
            print("Error: CSV file is empty.")
            return
    else:
        with open(args.input_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip(): data.append(json.loads(line))

    if args.limit:
        print(f"âš ï¸ Limit set to {args.limit}")
        data = data[:args.limit]

    print(f"processing {len(data)} items...")

    results = []

    # è¿›åº¦æ¡å¾ªç¯
    for idx, item in tqdm(enumerate(data), total=len(data)):
        drug = item.get('drug_name') or item.get('x_name')
        disease = item.get('disease_name') or item.get('y_name')
        if not drug or not disease: continue

        try:
            prob, text = get_local_logprobs(model, tokenizer, drug, disease)

            res = {
                "index": item.get('index', idx),
                "drug_name": drug,
                "disease_name": disease,
                "label": item.get('label'),
                "pred_prob": prob,
                "raw_response": text
            }
            results.append(res)
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"âŒ OOM Error at index {idx}. Skipping...")
                torch.cuda.empty_cache()  # å°è¯•æ¸…ç†æ˜¾å­˜
            else:
                print(f"âŒ Error at index {idx}: {e}")

        # æ¯10æ¡å­˜ç›˜ä¸€æ¬¡
        if len(results) % 10 == 0:
            pd.DataFrame(results).to_json(args.output_jsonl, orient='records', lines=True, force_ascii=False)

    # æœ€ç»ˆä¿å­˜
    pd.DataFrame(results).to_json(args.output_jsonl, orient='records', lines=True, force_ascii=False)
    print(f"ğŸ‰ Done! Output saved to {args.output_jsonl}")


if __name__ == "__main__":
    main()