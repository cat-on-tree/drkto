import pandas as pd
import numpy as np
from tqdm import tqdm
import os
from collections import defaultdict

# ==========================================
# 0. é…ç½®ä¸è·¯å¾„
# ==========================================
# æœŸæœ›é…ç½®
NUM_POS_SAMPLES = 100  # æœŸæœ›çš„æ­£æ ·æœ¬æ•°é‡
NEG_POS_RATIO = 10  # è´Ÿæ­£æ¯”ä¾‹ (1:10)
SEED = 42

# è·¯å¾„
input_clean_pos = "../../data/benchmark/Kaggle_drug_repositioning/full_mapping_without_na.csv"  # æ­£æ ·æœ¬æ¥æº
input_raw_full = "../../data/benchmark/Kaggle_drug_repositioning/full_mapping.csv"  # Ban Set æ¥æº
train_path = "../../data/benchmark/PrimeKG/train_edges.csv"
val_path = "../../data/benchmark/PrimeKG/val_edges.csv"
nodes_path = "../../data/benchmark/PrimeKG/nodes.csv"
save_dir = "../../data/benchmark/Kaggle_drug_repositioning"
os.makedirs(save_dir, exist_ok=True)

rng = np.random.default_rng(SEED)

print(f"ğŸš€ Generating STRICT Degree-Matched Test Set...")
print(f"   Target: {NUM_POS_SAMPLES} Positives, Ratio 1:{NEG_POS_RATIO}")

# ==========================================
# 1. å‡†å¤‡èŠ‚ç‚¹ç±»å‹ä¸åº¦æ•°ä¿¡æ¯
# ==========================================
print(">>> Step 1: Loading Nodes & Calculating Degrees...")
df_nodes = pd.read_csv(nodes_path)
valid_disease_ids = set(df_nodes[df_nodes['node_type'] == 'disease']['node_index'].unique())
valid_drug_ids = set(df_nodes[df_nodes['node_type'] == 'drug']['node_index'].unique())

# è¯»å–è®­ç»ƒ/éªŒè¯é›† (ç”¨äºè®¡ç®—åº¦æ•° + Ban Set)
df_train = pd.read_csv(train_path)
df_val = pd.read_csv(val_path)

# è®¡ç®—åº¦æ•° (ä»…åŸºäºå·²çŸ¥å›¾è°± Train+Val)
all_known_edges = pd.concat([df_train, df_val])
all_nodes_series = pd.concat([all_known_edges['x_index'], all_known_edges['y_index']])
degrees = all_nodes_series.value_counts().to_dict()

# æ„å»º {åº¦æ•°: [Valid Disease List]} æ˜ å°„è¡¨
degree_lookup = defaultdict(list)
for node in valid_disease_ids:
    d = degrees.get(node, 0)
    degree_lookup[d].append(node)
# è½¬ numpy array åŠ é€Ÿ
degree_lookup_np = {k: np.array(v) for k, v in degree_lookup.items()}

# ==========================================
# 2. æ„å»ºè¶…çº§ç¦å¿Œè¡¨ (Global Ban Set)
# ==========================================
print(">>> Step 2: Building Comprehensive Global Ban Set...")

# A. è®­ç»ƒé›† + éªŒè¯é›†
ban_sources = [df_train, df_val]

# B. å…¨é‡åŸå§‹æ•°æ® (ç”¨äºæ’é™¤æ½œåœ¨å‡è´Ÿä¾‹)
# å…³é”®å¤„ç†ï¼šåªæœ‰ x_index å’Œ y_index éƒ½å­˜åœ¨çš„è¡Œæ‰æœ‰èµ„æ ¼è¿›å…¥ Ban Set
df_raw = pd.read_csv(input_raw_full)
df_raw_valid = df_raw.dropna(subset=['x_index', 'y_index']).copy()
# ç¡®ä¿ç±»å‹ä¸º intï¼Œä»¥ä¾¿åç»­åŒ¹é…
df_raw_valid['x_index'] = df_raw_valid['x_index'].astype(int)
df_raw_valid['y_index'] = df_raw_valid['y_index'].astype(int)
ban_sources.append(df_raw_valid)

# åˆå¹¶æ‰€æœ‰æº
df_ban_all = pd.concat(ban_sources)

# æ„å»º Set: (min(u,v), max(u,v)) æ— å‘åŒ¹é…
# è¿™æ ·æœ€å®‰å…¨ï¼Œä¸ç®¡è°æ˜¯å¤´è°æ˜¯å°¾ï¼Œåªè¦è¿è¿‡å°±ä¸åšè´Ÿæ ·æœ¬
global_ban_set = set(zip(
    df_ban_all[['x_index', 'y_index']].min(axis=1),
    df_ban_all[['x_index', 'y_index']].max(axis=1)
))

print(f"   Total unique edges banned: {len(global_ban_set)}")

# ==========================================
# 3. é‡‡æ ·æ­£æ ·æœ¬ (æ¥è‡ª Clean Source)
# ==========================================
print(f">>> Step 3: Sampling {NUM_POS_SAMPLES} Positives...")
df_clean = pd.read_csv(input_clean_pos)
df_clean['x_index'] = df_clean['x_index'].astype(int)
df_clean['y_index'] = df_clean['y_index'].astype(int)

# è¿‡æ»¤ 1: ç±»å‹æ­£ç¡® (Drug -> Disease)
mask_type = df_clean['x_index'].isin(valid_drug_ids) & df_clean['y_index'].isin(valid_disease_ids)
candidates = df_clean[mask_type].copy()

# è¿‡æ»¤ 2: ä¸èƒ½åœ¨ Train/Val ä¸­ (é˜²æ­¢æ•°æ®æ³„æ¼)
# è¿™é‡Œç”¨ä¸€ä¸ªç®€å•çš„ set æŸ¥é‡
train_val_pairs = set(zip(df_train['x_index'], df_train['y_index'])) | \
                  set(zip(df_val['x_index'], df_val['y_index']))


def is_leak(row):
    return (row['x_index'], row['y_index']) in train_val_pairs


candidates['is_leak'] = candidates.apply(is_leak, axis=1)
clean_candidates = candidates[~candidates['is_leak']].copy()

# é‡‡æ ·
if len(clean_candidates) > NUM_POS_SAMPLES:
    test_pos = clean_candidates.sample(n=NUM_POS_SAMPLES, random_state=SEED).reset_index(drop=True)
else:
    test_pos = clean_candidates.reset_index(drop=True)

# æ ¼å¼åŒ–æ­£æ ·æœ¬
test_pos = test_pos[['x_index', 'y_index']].copy()
test_pos['label'] = 1
test_pos['relation'] = 'indication'

print(f"   Final Positives: {len(test_pos)}")

# ==========================================
# 4. ç”Ÿæˆåº¦åŒ¹é…è´Ÿæ ·æœ¬ (1:N)
# ==========================================
target_neg_count = len(test_pos) * NEG_POS_RATIO
print(f">>> Step 4: Generating Negatives (Target: {target_neg_count})...")

hard_neg_rows = []
tolerance_levels = [0.0, 0.05, 0.1, 0.2, 0.5, 1.0]

for _, row in tqdm(test_pos.iterrows(), total=len(test_pos)):
    src = int(row['x_index'])  # Drug
    dst_real = int(row['y_index'])  # Disease

    target_deg = degrees.get(dst_real, 0)
    created_count = 0
    retry = 0

    # å°è¯•ç”Ÿæˆ N ä¸ªè´Ÿæ ·æœ¬
    while created_count < NEG_POS_RATIO and retry < 50:
        found_one = False

        for tol in tolerance_levels:
            min_d = int(target_deg * (1 - tol))
            max_d = int(target_deg * (1 + tol))

            candidates_list = [degree_lookup_np[d] for d in range(min_d, max_d + 1) if d in degree_lookup_np]
            if not candidates_list: continue

            pool = np.concatenate(candidates_list)

            # åœ¨å½“å‰å®¹å¿åº¦ä¸‹å°è¯• 10 æ¬¡
            for _ in range(10):
                fake_dst = int(rng.choice(pool))

                if fake_dst == dst_real: continue

                # ğŸ”¥ æŸ¥ Global Ban Set
                check_pair = (min(src, fake_dst), max(src, fake_dst))

                if check_pair not in global_ban_set:
                    hard_neg_rows.append({
                        'relation': 'indication',
                        'x_index': src,
                        'y_index': fake_dst,
                        'label': 0
                    })
                    created_count += 1
                    found_one = True
                    break

            if found_one: break  # è·³å‡º tol å¾ªç¯ï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªè´Ÿæ ·æœ¬

        if not found_one: retry += 1

test_neg = pd.DataFrame(hard_neg_rows)

# ==========================================
# 5. ä¿å­˜
# ==========================================
print(">>> Step 5: Saving...")
# åˆå¹¶
test_final = pd.concat([test_pos, test_neg], ignore_index=True)
test_final = test_final.sample(frac=1, random_state=SEED).reset_index(drop=True)

# ç¡®ä¿åˆ—é¡ºåºå’Œç±»å‹
cols = ['relation', 'x_index', 'y_index', 'label']
test_final = test_final[cols]
test_final['x_index'] = test_final['x_index'].astype(int)
test_final['y_index'] = test_final['y_index'].astype(int)
test_final['label'] = test_final['label'].astype(int)

# ä¿å­˜
filename = "test_hard.csv"
output_path = os.path.join(save_dir, filename)
test_final.to_csv(output_path, index=False)

print(f"ğŸ‰ Success! Saved to {output_path}")
print(f"   Positives: {len(test_pos)}")
print(f"   Negatives: {len(test_neg)}")
print(f"   Ratio: 1:{len(test_neg) / len(test_pos):.2f}")